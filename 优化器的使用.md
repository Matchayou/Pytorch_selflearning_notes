---
date: 2025-08-12
tags:
  - 代码
  - 算法
  - pytorch
  - 优化器
---

# 优化器如何调整模型损失（`result_loss`）

### 假设数据

假设我们有两个批次的数据，分别是 `data1` 和 `data2`，每个批次包含 4 张图像和对应的标签：

- `data1`：4 张图像，标签 `[0, 1, 2, 3]`
- `data2`：4 张图像，标签 `[1, 2, 3, 0]`
### 1. 初始状态
在开始训练之前，模型的参数是随机初始化的，假设其初始值如下：
- 权重：`w1`, `w2`, …（随机初始化）
- 偏置：`b1`, `b2`, …（随机初始化）
### 2. 第一个批次 `data1`
假设经过模型前向传播，得到以下输出（每个批次的 4 张图像对应 4 个预测值）：
- `data1`（4 张图像的预测输出）：
  - 图像 1：`[0.3, 1.2, 0.5]`（模型对图像 1 的三个类别预测的概率分布）
  - 图像 2：`[0.6, 0.4, 1.5]`
  - 图像 3：`[1.0, 0.2, 0.3]`
  - 图像 4：`[0.4, 1.0, 0.5]`
- 标签：`[0, 1, 2, 3]`
然后，计算 `result_loss`：
```python
result_loss = loss(outputs, targets)  
# 计算交叉熵损失
```
- `outputs` 是模型对 `data1` 中每张图像的预测（每个图像 3 类的概率分布）。
- `targets` 是标签 `[0, 1, 2, 3]`，表示每张图像的真实类别。

假设计算出的损失是 2.3。也就是说，当前模型在 `data1` 上的表现较差，损失较高。
#### 反向传播 (`result_loss.backward()`)

接着，通过 `result_loss.backward()` 来计算每个模型参数（如卷积层、全连接层的权重）的梯度。每个参数的梯度是通过反向传播计算得出的，表示该参数的调整方向和幅度（以减小损失为目标）。
### 3. 更新模型参数 (`optim.step()`)

优化器通过调用 `optim.step()` 更新模型的参数：

```python
optim.step()  # 更新参数
```
- SGD 优化器根据计算出的梯度和学习率来调整权重和偏置。假设学习率为 0.01，SGD 的更新规则是：
公式：
$$
w_{\text{new}} = w_{\text{old}} - \eta \cdot \frac{\partial \text{Loss}}{\partial w}
$$
  其中，`η` 是学习率，`∂Loss/∂w` 是该参数的梯度。
通过这一步，模型的参数会根据梯度的方向和大小被更新，以减小损失。
### 4. 第二个批次 `data2`
接着，模型会继续训练，处理下一个批次 `data2`，此时参数已经更新。假设经过前向传播，得到以下输出：
- `data2`（4 张图像的预测输出）：
  - 图像 1：`[0.5, 1.0, 0.2]`
  - 图像 2：`[0.7, 0.3, 1.0]`
  - 图像 3：`[0.8, 0.2, 0.4]`
  - 图像 4：`[0.3, 1.1, 0.6]`
- 标签：`[1, 2, 3, 0]`
然后，计算第二个批次的 `result_loss`：

```python
result_loss = loss(outputs, targets)  
# 计算交叉熵损失
```
- `outputs` 是模型对 `data2` 中每张图像的预测。
- `targets` 是标签 `[1, 2, 3, 0]`。
假设计算出的损失是 1.5，比第一次的 2.3 略小。
### 5. 第二次反向传播与参数更新

通过 `result_loss.backward()` 计算第二个批次的梯度，表示如何调整参数以减少损失。

接着，使用 `optim.step()` 来更新参数。模型根据第二个批次的数据，继续调整参数，以使损失进一步减小。

### 6. 损失 (`result_loss`) 的变化

- **`data1`** 中的损失是 2.3（初始阶段，模型还没有调整好）。
- 经过 `optim.step()` 更新参数后，模型在 **`data2`** 上的损失下降到 1.5，说明模型已经在一定程度上学习到了如何改进预测。

在整个训练过程中，`result_loss` 会随着每个批次的数据不断变化：
- 初始时，`result_loss` 较高，表示模型还没有学好。
- 每经过一次反向传播和参数更新，`result_loss` 会逐渐减小，表示模型在逐步改善预测能力。
### **总结：**

1. **初始状态**：模型参数是随机初始化的，`result_loss` 较大。
2. **第一批次 `data1`**：计算出损失，反向传播，优化器通过 `optim.step()` 更新参数。
3. **第二批次 `data2`**：由于参数已经更新，模型的预测性能有所改善，`result_loss` 较第一次下降。
4. **持续训练**：随着更多批次的训练，`result_loss` 会继续下降，模型逐步优化。

### 代码笔记
