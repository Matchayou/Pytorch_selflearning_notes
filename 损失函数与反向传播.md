## CrossEntropyLoss
### 1. 数学原理
**交叉熵损失**的目标是最大化真实标签对应的类别的预测概率，最小化其他类别的概率。其核心思想是：
- 对于每个样本，计算模型的预测类别概率分布。
- 然后计算这个预测概率分布与真实标签（通常是独热编码形式）之间的差异。
- 损失越小，表示模型越准确，预测的类别概率与真实标签越接近。
### 2. 公式描述
交叉熵损失的计算可以分为两个部分：
1. **Softmax 操作**：将未归一化的 logits 转换为类别的概率分布。
2. **负对数似然损失**（NLLLoss）：用来计算预测的概率分布与真实标签之间的差异。
假设输入为 $x_n=[x_{n,1},x_{n,2},...,x_{n,C}]$（第 n个样本的 logits，其中 C 为类别数），真实标签为 $y_n$（类别索引，表示该样本属于哪个类别）。
$$
\ell(x, y) = - \sum_{c=1}^{C} w_c \cdot \log\left(\frac{\exp(x_{n,{y_n}})}{\sum_{c=1}^{C} \exp(x_{n,c})}\right)
$$

其中：
- $x_{n,c}$ 是第 \(n\) 个样本对于类别 \(c\) 的原始预测值（logits）。
- $y_n$ 第 \(n\) 个样本的真实标签
- $w_{y_n}$是类别 \(c\) 的权重，用来调整不同类别的重要性，通常用于不平衡类别的问题。
- \( C \) 是类别数，\( N \) 是样本数。
**损失计算过程**：

1. **Softmax** 计算每个类别的概率：
    $$p_{n,c} =  \frac{\exp(x_{n,c})}{\sum_{c=1}^{C} \exp(x_{n,i})}$$
2. **对数概率**：计算模型预测的类别 $y_n$​ 对应的对数概率：
    $$\log\left(p_{n,c}\right)= - \ \ \log\left(\frac{\exp(x_{n,{y_n}})}{\sum_{c=1}^{C} \exp(x_{n,i})}\right)$$
3. **损失**：然后将对数概率与类别权重相乘，得到每个样本的损失：
    $$l_n=- w_{y_n} \cdot \log\left(p_{n,{y_n}}\right)$$
**交叉熵损失**广泛应用于以下场景：
- **分类任务**：用于图像分类、文本分类等多分类问题。
- **类别不平衡**：当某些类别的样本远多于其他类别时，可以通过设置 `weight` 参数对每个类别的损失进行加权，避免模型偏向频繁出现的类别。
- **标签平滑**：在标签平滑的情况下，目标不再是一个硬标签（如 `y = [0, 1, 0]`），而是每个类别有一个概率分布。交叉熵损失会根据这些概率计算损失，从而降低模型对某一类别的过拟合。
### **4. `CrossEntropyLoss` 类的运作**
在 **PyTorch** 中，`CrossEntropyLoss` 函数会执行以下操作：
1. **Softmax 转换**：首先，它将输入的 logits（未经归一化的类别分数）通过 Softmax 转换为概率分布。
2. **计算损失**：然后，计算模型预测的概率与真实标签之间的交叉熵损失。对于每个样本，损失会基于目标的类别标签计算。
#### **常见参数：**
- **`weight`**：当某些类别样本较少时，可以通过设置 `weight` 参数来对不同类别赋予不同的权重，帮助平衡类别。
- **`reduction`**：设置损失计算的方式。常见的选项有：
    - `'mean'`：返回所有样本损失的平均值（默认）。
    - `'sum'`：返回所有样本损失的总和。
    - `'none'`：返回每个样本的单独损失。
- **`ignore_index`**：可以指定一个类别索引（如填充标签）来忽略该类别的损失。
##### 5. 数学推导的示例
假设有 3 个类别，一个 mini-batch 中有 2 个样本。假设第一个样本的 logits 为 `[2.0, 1.0, 0.1]`，第二个样本的 logits 为 `[1.2, 2.5, 0.3]`。目标标签分别是类别 0 和类别 2。
1. **Softmax 转换**：  
    对于第一个样本，Softmax 计算每个类别的概率：
    $p_{1,0}​=\frac{exp(2.0)}{exp(1.0)+exp(0.1)exp(2.0)}​=\frac{7.389}{2.718+1.105+7.389​}≈0.659$
    $p_{1,1}​=\frac{exp(1.0)}{exp(1.0)+exp(0.1)exp(2.0)}​=\frac{2.718}{2.718+1.105+7.389​}≈0.242$
    $p_{1,2}​=\frac{exp(0.1)}{exp(1.0)+exp(0.1)exp(2.0)}​=\frac{1.105}{2.718+1.105+7.389​}≈0.099$
    所以，第一个样本的概率分布为：
							$p_1=[0.659,0.242,0.098]$
对于第二个样本 $x_2=[1.2,2.5,0.3]$，我们也进行 Softmax 计算：
$p_{2,0} = \frac{\exp(1.2)}{\exp(1.2) + \exp(2.5) + \exp(0.3)} = \frac{3.320116}{3.320116 + 12.182493 + 1.349859} \approx \frac{3.320116}{16.852468} \approx 0.197$
$p_{2,1} = \frac{\exp(2.5)}{\exp(1.2) + \exp(2.5) + \exp(0.3)} = \frac{12.182493}{16.852468} \approx 0.723$
$p_{2,2} = \frac{\exp(0.3)}{\exp(1.2) + \exp(2.5) + \exp(0.3)} = \frac{1.349859}{16.852468} \approx 0.080$
所以，第二个样本的概率分布为：
$p_2 = [0.197, 0.723, 0.080]$
2. **计算损失**：  
    对于第一个样本，类别标签是 0，所以损失是：
    $l_1=−log⁡(p_{1,0}) = −log(0.659)≈0.416$
    类似地，对于第二个样本，计算其损失。
    $l_2​=−log(p_{2,2}​)=−log(0.080)≈2.525$
3. **计算总损失**：  
    如果 `reduction='mean'`，则对所有样本的损失取平均：
    $总损失=ℓ(x,y)= \frac{1}{N} \sum_{n=1}^{N} l_n​ =\frac{1}{2}​(0.416+2.525)=1.4705$
    如果 `reduction='sum'`，则对所有样本的损失求和。
	$ℓ(x,y)=0.416+2.525=2.941$
### Softmax 和 交叉熵
#### 1. **Softmax**：
   - Softmax 是一种函数，通常用于多分类任务的输出层，它将一个向量的每个元素转化为 0 到 1 之间的概率值，且所有元素的概率和为 1。
   - 其公式为：
$$ 
     \text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
     $$
     其中$z_i$ 是输入向量中的第 \(i\) 个元素， \(j\) 是所有元素的索引。
   - Softmax 的作用是将神经网络的输出转换为概率分布，用于多分类问题的预测。
**解释：**
1. **指数变换**：对每个元素 $z_i$ ​ 进行指数运算，得到 $e^{z_i}$ 。指数运算的作用是放大输入向量中的大数，使它们的值变得更大；相对地，较小的值会变得更小。这意味着 Softmax 会更加强调较大的输入值（这对于多分类任务中的预测很有用）。
2. **归一化**：通过将每个$e^{z_i}$除以所有元素指数的总和（即 ${\sum_{j} e^{z_j}}$，Softmax 将每个元素的值归一化，使得所有的输出值的总和为 1。
	**归一化的作用是保证输出值是一个概率分布，这样每个值都表示输入中某个类别的概率，且这些概率的和为 1，符合概率分布的性质。**

#### 2. **交叉熵（Cross-Entropy）**：
   - 交叉熵是衡量两个概率分布之间差异的一个指标，常用于计算模型预测分布与真实标签分布之间的差异。
   - 在分类任务中，交叉熵损失函数可以表示为：
  $$ 
     H(p, q) = - \sum_{i} p(x_i) \log q(x_i)
     $$
     其中 $p(xi)$ 是真实分布，$q(xi)$是预测分布（通常通过 Softmax 输出），$xi$ 是类别标签。
   - 交叉熵的作用是衡量模型输出的概率分布与真实标签分布的距离，越小表示预测越准确。

### 在分类任务中的结合：
- 在多分类问题中，网络的最后一层通常是 Softmax，它将输出层的 logits 转化为概率分布。
- 交叉熵损失函数则用来计算预测概率分布与真实标签之间的误差，帮助模型更新参数。

简而言之：
- **Softmax** 将模型的输出转化为概率。
- **交叉熵** 衡量预测概率与真实标签之间的差异。
